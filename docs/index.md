# Weights & Biases Documentation

## Intro

[Weights & Biases](http://wandb.com) tracks machine learning jobs in real-time, makes them reproducible, and permanently archives your results (like trained models).


## Quickstart - Existing Project

This explains how to quickly integrate wandb into an existing project.

<br>
Aftering [signing up](https://app.wandb.ai/login), install the Weights & Biases command line tool "wandb":
```console
$ pip install wandb
```

<br>
Initialize Weights & Biases in your project:
```console
$ cd <project_directory>
$ wandb init
```

Follow the prompts to complete the initialization process.

<br>
Then, import our Python module into your code. In your training script:
```python
import wandb
```

<br>
Finally, launch your job:
```console
wandb run --show <train.py>
```

## Weights & Biases Run API

Your training jobs can interact with Weights & Biases via the `wandb.run` object. The members of `wandb.run` are described in the following sections.

### wandb.run.dir

This is the path to your Weights & Biases run directory, which by default will be a subdirectory of `./wandb`. Any files you save in this directory during the run will be persisted to Weights & Biases. We recommend that you modify your training script to save generated models and other run artifacts in this directory.

#### Aside: Saving all generated files without modifying your script

You can use `wandb run --dir=. <script.py>` to make wandb sync _all_ files (in the current directory and all descendent directories) generated by your script. This isn't the preferred approach because it won't work well for running parallel instances of your script on a single machine, but it can be useful in some cases.

#### Dry run mode

If you launch your script directly (`python <script.py>` or `./script.py`) instead of using `wandb run <script.py>`, it will be launched in dry run mode, wherein nothing is persisted to wandb. We still create a run directory and other `wandb.run` members, so your script will function normally. We recommend using dry run mode while developing your script.


### wandb.run.config

The config object is a dict-like that serves two purposes: 1) whatever you put into it will be persisted as the configuration for this run, 2) `wandb run` can optionally read in values from yaml files, and pass them to your script via `wandb.run.config`.


#### Saving parameters from argparse/tensorflow/other

If you are using argparse you can easily persist your arguments:
```python
parser = argparse.ArgumentParser()
parser.add_argument('--epochs', type=int, default=10)
args = parser.parse_args()

wandb.run.config.update(args)
```

If you're using tensorflow's flags, you can easily persist those:
```python
flags = tf.app.flags
flags.DEFINE_integer('epochs', 10, 'Number of epochs')

wandb.run.config.update(flags.FLAGS)
```

You can also set individual keys or attributes on the object.

#### Using yaml files to configure your runs

You can use the wandb config system to pass in configurations from yaml files, rather than via command line parameters. Or you can use both: yaml files to pass in defaults, and command line parameters to override them.

`wandb run` will always look for a file called "config-defaults.yaml" in the root directory of your project. If "config-defaults.yaml" is present it populates the contained values in wandb.run.config when your run starts. This file is generated when you run `wandb init`, and contains some example variables.

You can pass extra config files with `wandb run --configs=<config_files>` where `<config_files>` is a comma-separated list of paths to yaml files. These will be processed in order, with values from later files overriding values from earlier files.

### wandb.run.history / wandb.run.summary

wandb's history and summary allow you to save data about your training run.

#### wandb.run.history

history is for storing data that changes over time. For example, if your run proceeds by epochs, you should probably save one data point per epoch, with loss, validation accuracy and other metrics. Just call `wandb.run.history.add(<data>)` for each epoch, where `<data>` is a dict of `key: value` pairs.

Weights & Biases shows plots of this data in real-time as your job runs.

#### wandb.run.summary

This is a dict-like object used to summarize the results of your run. We recommend storing information that you'd use to compare this run to other runs. For example, you could store the highest accuracy that your run achieved.

Weights & Biases shows these values in the runs page for your project.


#### Automatically generating history & summary with Keras

You can use `wandb_keras.WandbKerasCallback()` to automatically generate summary and history data. It will also save the best model, like `keras.ModelCheckpoint`. Just instantiate it and pass it in to Keras' `model.fit` as a callback.